{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["caA2JdQvMG0a"],"authorship_tag":"ABX9TyM0eD1Edj3d/gFKYCQbki60"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8msvfj0KkEF","executionInfo":{"status":"ok","timestamp":1737887553314,"user_tz":-540,"elapsed":59793,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"69ae4736-eb5e-44a5-e0b8-763e87c4211d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"uKGvQ01WKnmc","executionInfo":{"status":"ok","timestamp":1737887558942,"user_tz":-540,"elapsed":545,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Increase display width for columns\n","pd.set_option('display.max_colwidth', None)"],"metadata":{"id":"s3QASrlMK6Oc","executionInfo":{"status":"ok","timestamp":1737887578219,"user_tz":-540,"elapsed":604,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/github/book_recommendation/book_recommender'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3zWm-83aLNi2","executionInfo":{"status":"ok","timestamp":1737887710663,"user_tz":-540,"elapsed":7,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"2def0f8f-67df-49aa-e8a5-d3a84429e1d8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/github/book_recommendation/book_recommender\n"]}]},{"cell_type":"code","source":["data = pd.read_csv(f'books_summary.csv')"],"metadata":{"id":"tPOnX8P9K-6Z","executionInfo":{"status":"ok","timestamp":1737887717986,"user_tz":-540,"elapsed":1131,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Drop missing values\n","data = data.dropna(subset=['book_name', 'summaries'])\n","data = data.reset_index(drop=True)\n","# Remove duplicates where both book_name and categories are identical\n","data = data.drop_duplicates(subset=['book_name', 'categories'], keep='first').reset_index(drop=True)\n","# Group categories for each book\n","data = data.groupby('book_name', as_index=False).agg({\n","    'summaries': 'first',  # Retain the first summary\n","    'categories': ', '.join  # Combine categories into a single string\n","})\n","data['categories_list'] = data['categories'].str.split(', ')\n","# Combine summaries with joined categories for each row\n","data['combined_text'] = data.apply(\n","    lambda row: row['summaries'] + \" \" + \" \".join(row['categories_list']),\n","    axis=1\n",")"],"metadata":{"id":"ccefnGrULLU6","executionInfo":{"status":"ok","timestamp":1737887790975,"user_tz":-540,"elapsed":633,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!pip install sentence-transformers datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"itx8HY0XLzHC","executionInfo":{"status":"ok","timestamp":1737887821608,"user_tz":-540,"elapsed":6646,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"95b45e43-d1af-4054-da09-aa15fc04f5a7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n","Collecting datasets\n","  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["from datasets import Dataset\n","from sentence_transformers import SentenceTransformer"],"metadata":{"id":"Kvne9P5gL5GZ","executionInfo":{"status":"ok","timestamp":1737887878337,"user_tz":-540,"elapsed":27364,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tuning the sentence Bert model on book summaries"],"metadata":{"id":"caA2JdQvMG0a"}},{"cell_type":"code","source":["data['embeddings'] = data['summaries'].apply(lambda x: model.encode(x, convert_to_tensor=True))"],"metadata":{"id":"TnMKM20gL8N5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import combinations\n","from sentence_transformers import SentenceTransformer, util\n","\n","# Load pre-trained SBERT model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","def generate_pairs(data, num_samples=1000):\n","    pairs = []\n","    sampled_combinations = combinations(data.iterrows(), 2)\n","    for (idx1, row1), (idx2, row2) in sampled_combinations:\n","        # Compute Jaccard similarity\n","        common_categories = len(set(row1['categories_list']) & set(row2['categories_list']))\n","        total_categories = len(set(row1['categories_list']) | set(row2['categories_list']))\n","        jaccard_similarity = common_categories / total_categories\n","\n","\n","        # Reuse precomputed embeddings\n","        semantic_similarity = util.pytorch_cos_sim(row1['embeddings'], row2['embeddings']).item()\n","\n","        # Final similarity (weighted average)\n","        combined_similarity = 0.9 * semantic_similarity + 0.1 * jaccard_similarity\n","\n","        # Append the pair with book names\n","        pairs.append({\n","            \"book1\": row1['book_name'],  # Book name for text1\n","            \"book2\": row2['book_name'],  # Book name for text2\n","            \"text1\": row1['summaries'] ,\n","            \"text2\": row2['summaries'] ,\n","            \"similarity\": combined_similarity\n","        })\n","\n","        # Stop if we've generated enough samples\n","        #if len(pairs) >= num_samples:\n","        #    break\n","\n","    return pd.DataFrame(pairs)\n","\n","# Generate pairs\n","pairs_df = generate_pairs(data)\n","\n","# Display the first few rows\n","#print(pairs_df.head())\n"],"metadata":{"id":"y0iBoNSfMQMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.utils import shuffle\n","from sentence_transformers import SentenceTransformer, InputExample\n","from torch.utils.data import DataLoader\n","\n","# Define bins for similarity scores\n","def stratify_data(pairs_df, high_threshold=0.5, low_threshold=0.3, samples_per_bin=5000):\n","    \"\"\"\n","    Stratifies pairs_df into bins based on similarity scores and samples equally from each bin.\n","\n","    Args:\n","        pairs_df (pd.DataFrame): DataFrame containing the similarity scores.\n","        high_threshold (float): Threshold for high similarity.\n","        low_threshold (float): Threshold for low similarity.\n","        samples_per_bin (int): Number of samples to draw from each bin.\n","\n","    Returns:\n","        pd.DataFrame: Stratified and sampled DataFrame.\n","    \"\"\"\n","    # Define bins\n","    high_similarity = pairs_df[pairs_df['similarity'] >= high_threshold]\n","    moderate_similarity = pairs_df[(pairs_df['similarity'] < high_threshold) & (pairs_df['similarity'] >= low_threshold)]\n","    low_similarity = pairs_df[pairs_df['similarity'] < low_threshold]\n","\n","    # Sample equally from each bin\n","    high_sample = high_similarity.sample(min(len(high_similarity), samples_per_bin), random_state=42)\n","    moderate_sample = moderate_similarity.sample(min(len(moderate_similarity), samples_per_bin), random_state=42)\n","    low_sample = low_similarity.sample(min(len(low_similarity), samples_per_bin), random_state=42)\n","\n","    # Combine samples and shuffle\n","    stratified_data = pd.concat([high_sample, moderate_sample, low_sample])\n","    return shuffle(stratified_data, random_state=42)\n","\n","# Apply stratification\n","stratified_pairs_df = stratify_data(pairs_df, samples_per_bin=5000)\n","\n","# Convert stratified pairs to InputExamples\n","train_examples = [\n","    InputExample(texts=[row['text1'], row['text2']], label=float(row['similarity']))\n","    for _, row in stratified_pairs_df.iterrows()\n","]\n","\n","# Create DataLoader\n","train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=128)\n","\n","# Check stratification\n","print(\"High Similarity Samples:\", len(stratified_pairs_df[stratified_pairs_df['similarity'] >= 0.5]))\n","print(\"Moderate Similarity Samples:\", len(stratified_pairs_df[(stratified_pairs_df['similarity'] < 0.5) & (stratified_pairs_df['similarity'] >= 0.3)]))\n","print(\"Low Similarity Samples:\", len(stratified_pairs_df[stratified_pairs_df['similarity'] < 0.3]))\n"],"metadata":{"id":"Z5j_EMFiMQPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = SentenceTransformer('all-MiniLM-L6-v2')\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""],"metadata":{"id":"X9f6iQACMWrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import losses\n","\n","# Define the loss function\n","train_loss = losses.CosineSimilarityLoss(model)\n","\n","# Fine-tune the model\n","model.fit(\n","    train_objectives=[(train_dataloader, train_loss)],\n","    epochs=10,\n","    warmup_steps=int(0.01 * len(train_dataloader)),  # 10% warmup\n","    output_path=f'{path}fine_tuned_sbert'\n",")\n"],"metadata":{"id":"nvspLjhhMWuB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recommendation function"],"metadata":{"id":"ZuveP3ACMgcR"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import gradio as gr\n","from rapidfuzz import process\n","\n","\n","### Step 1: Data Loading ###\n","def load_data(file_path):\n","    try:\n","        data = pd.read_csv(file_path)\n","        if not all(col in data.columns for col in ['book_name', 'summaries', 'categories']):\n","            raise ValueError(\"Dataset must contain 'book_name', 'summaries', and 'categories' columns.\")\n","        return data\n","    except Exception as e:\n","        raise FileNotFoundError(f\"Error loading file: {e}\")\n","\n","\n","### Step 2: Preprocessing ###\n","def preprocess_data(data):\n","    data = data.dropna(subset=['book_name', 'summaries']).reset_index(drop=True)\n","    data = data.drop_duplicates(subset=['book_name', 'categories'], keep='first').reset_index(drop=True)\n","    data = data.groupby('book_name', as_index=False).agg({\n","        'summaries': 'first',\n","        'categories': ', '.join\n","    })\n","    data['categories_list'] = data['categories'].str.split(', ')\n","    data['combined_text'] = data.apply(\n","        lambda row: row['summaries'] + \" \" + \" \".join(row['categories_list']),\n","        axis=1\n","    )\n","    return data\n","\n","\n","### Step 3: Embedding Loading ###\n","def load_embeddings(embedding_path):\n","    try:\n","        embeddings = np.load(embedding_path)\n","        return embeddings\n","    except Exception as e:\n","        raise FileNotFoundError(f\"Error loading embeddings: {e}\")\n","\n","\n","### Step 4: Recommendation Generation ###\n","\n","from rapidfuzz import process\n","\n","def recommend_books_with_category_filter(book_title, data, embeddings, top_n=5):\n","    # Normalize book titles to lowercase\n","    book_title = book_title.lower()\n","    data['book_name'] = data['book_name'].str.lower()\n","\n","    # Check for exact match\n","    if book_title not in data['book_name'].values:\n","        # Use fuzzy matching to find the closest match\n","        closest_match = process.extractOne(book_title, data['book_name'].values)\n","        if closest_match is None or closest_match[1] < 70:  # Set a threshold for similarity\n","            return [\"Book not found in the dataset.\"]\n","        book_title = closest_match[0]  # Use the closest matching book name\n","        print(f\"Giving results for: {book_title}\")\n","\n","    # Find the index of the input book\n","    input_idx = data[data['book_name'] == book_title].index[0]\n","    input_embedding = embeddings[input_idx]\n","    input_categories = set(data.loc[input_idx, 'categories_list'])\n","\n","    # Compute cosine similarity\n","    similarity_scores = cosine_similarity([input_embedding], embeddings).flatten()\n","    similarity_scores[input_idx] = -1  # Exclude the input book\n","\n","    # Add similarity scores to a copy of the data\n","    data_copy = data.copy()\n","    data_copy['similarity'] = similarity_scores\n","\n","    # Filter books by category overlap\n","    data_filtered = data_copy[data_copy['categories_list'].apply(lambda x: len(set(x) & input_categories) > 0)]\n","\n","    # Sort by similarity score and select top_n recommendations\n","    recommended_books = data_filtered.sort_values(by='similarity', ascending=False).head(top_n)\n","\n","    return recommended_books[['book_name', 'similarity']].values.tolist()\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Ij6q1MCnMWwv","executionInfo":{"status":"ok","timestamp":1737900710282,"user_tz":-540,"elapsed":805,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["\n","### Main Workflow ###\n","# Load data and embeddings\n","data = preprocess_data(load_data('books_summary.csv'))\n","data['book_name'] = data['book_name'].str.lower()\n","embeddings = load_embeddings('book_embeddings.npy')"],"metadata":{"id":"NTqEodA4FqGD","executionInfo":{"status":"ok","timestamp":1737903101942,"user_tz":-540,"elapsed":607,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["def recommend_books_with_category_filter(book_title, data, embeddings, top_n=5, min_similarity=60):\n","    # Normalize book titles to lowercase\n","    book_title = book_title.lower()\n","\n","\n","    # Adjust the similarity threshold for numeric titles\n","    if book_title.isdigit():\n","        min_similarity = 50  # Lower threshold for numeric titles\n","\n","    if book_title not in data['book_name'].values:\n","        # Use token_sort_ratio from fuzz module\n","        closest_match = process.extractOne(\n","            book_title,\n","            data['book_name'].values,\n","            scorer=fuzz.token_sort_ratio\n","        )\n","\n","        if closest_match is None or closest_match[1] < min_similarity:\n","            return [f\"No close match found for '{book_title}'. Please try another title.\"]\n","\n","        book_title = closest_match[0]  # Use the closest matching book name\n","        print(f\"Giving results for: {book_title}\")\n","\n","    # Find the index of the input book\n","    input_idx = data[data['book_name'] == book_title].index[0]\n","    input_embedding = embeddings[input_idx]\n","    input_categories = set(data.loc[input_idx, 'categories_list'])\n","\n","    # Compute cosine similarity\n","    similarity_scores = cosine_similarity([input_embedding], embeddings).flatten()\n","    similarity_scores[input_idx] = -1\n","\n","    data_copy = data.copy()\n","    data_copy['similarity'] = similarity_scores\n","\n","    data_filtered = data_copy[data_copy['categories_list'].apply(lambda x: len(set(x) & input_categories) > 0)]\n","    recommended_books = data_filtered.sort_values(by='similarity', ascending=False).head(top_n)\n","\n","    return recommended_books[['book_name', 'similarity']].values.tolist()\n","\n","### Main Workflow ###\n","\n"],"metadata":{"id":"GWgDQnex-f9d","executionInfo":{"status":"ok","timestamp":1737903254111,"user_tz":-540,"elapsed":4,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["book_title = ' a tale of 5  cities'\n","recommendations = recommend_books_with_category_filter(book_title, data, embeddings, top_n=5)\n","if len(recommendations)<2:\n","    print(\"Book not found in the dataset. Please try another title.\")\n","else:\n","  print([f\"{rec[0]} (Similarity: {rec[1]:.4f})\" for rec in recommendations])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9U7EP7aIMWz0","executionInfo":{"status":"ok","timestamp":1737903523889,"user_tz":-540,"elapsed":576,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"18b7a193-a089-4901-99fa-22c015a15ca5"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["Giving results for: a tale of two cities\n","['happy together (Similarity: 0.5046)', 'say nothing (Similarity: 0.4844)', 'the social animal (Similarity: 0.4583)', 'the drama of the gifted child (Similarity: 0.4539)', 'the great gatsby is an american classic following jay gatsby’s quest to win back his long-lost love by faking a successful life,\\xa0 (Similarity: 0.4533)']\n"]}]},{"cell_type":"code","source":["books = data['book_name'].values[-2:]"],"metadata":{"id":"hyPYYrDYuWjF","executionInfo":{"status":"ok","timestamp":1737903400388,"user_tz":-540,"elapsed":563,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["for book in books:\n","  print(book)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMXboKSi_bbp","executionInfo":{"status":"ok","timestamp":1737903404652,"user_tz":-540,"elapsed":689,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"0a1701ca-9076-44fc-b800-d47d707331ad"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["zero to one\n","iwoz\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VRdzjfWoG-CR"},"execution_count":null,"outputs":[]}]}