{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1KjL3hHtARe0xZ2PhpwphA3ZLrgY0d5zh","authorship_tag":"ABX9TyOh4WHX/3dVTrwPiLZJ1nYR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8msvfj0KkEF","executionInfo":{"status":"ok","timestamp":1737977396306,"user_tz":-540,"elapsed":120608,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"ac4b0db3-670b-4900-9bf6-a24487a70faf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install rapidfuzz gradio sentence-transformers datasets faiss-cpu"],"metadata":{"collapsed":true,"id":"ZVhQmj82HxXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from rapidfuzz import process\n","import numpy as np\n","import gradio as gr\n","from datasets import Dataset\n","import faiss\n","from itertools import combinations\n","from sentence_transformers import SentenceTransformer, util , InputExample\n","from sentence_transformers import losses\n","from sklearn.utils import shuffle\n","from torch.utils.data import DataLoader"],"metadata":{"id":"uKGvQ01WKnmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yherVm7PitI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Increase display width for columns\n","pd.set_option('display.max_colwidth', None)"],"metadata":{"id":"s3QASrlMK6Oc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/github/book_recommendation/book_recommender'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3zWm-83aLNi2","executionInfo":{"status":"ok","timestamp":1737960875161,"user_tz":-540,"elapsed":392,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"5bd74e07-48ae-42b6-8a29-d76f60d97e06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/github/book_recommendation/book_recommender\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tPOnX8P9K-6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(file_path):\n","    \"\"\"\n","    Load the dataset from a CSV file.\n","    Args:\n","        file_path (str): Path to the CSV file.\n","    Returns:\n","        pd.DataFrame: Loaded dataset.\n","    \"\"\"\n","    try:\n","        data = pd.read_csv(file_path)\n","        if not all(col in data.columns for col in ['book_name', 'summaries', 'categories']):\n","            raise ValueError(\"Dataset must contain 'book_name', 'summaries', and 'categories' columns.\")\n","        return data\n","    except Exception as e:\n","        raise FileNotFoundError(f\"Error loading file: {e}\")\n","\n","def preprocess_data(data):\n","    \"\"\"\n","    Preprocess the dataset by:\n","    - Dropping rows with missing values in critical columns.\n","    - Removing duplicates based on 'book_name' and 'categories'.\n","    - Grouping categories for each book.\n","    - Adding a combined text column with summaries and categories.\n","\n","    Args:\n","        data (pd.DataFrame): Raw dataset.\n","\n","    Returns:\n","        pd.DataFrame: Preprocessed dataset.\n","    \"\"\"\n","    # Drop rows with missing values in 'book_name' and 'summaries'\n","    data = data.dropna(subset=['book_name', 'summaries']).reset_index(drop=True)\n","\n","    # Remove duplicates where both 'book_name' and 'categories' are identical\n","    data = data.drop_duplicates(subset=['book_name', 'categories'], keep='first').reset_index(drop=True)\n","\n","    # Group categories for each book\n","    data = data.groupby('book_name', as_index=False).agg({\n","        'summaries': 'first',  # Retain the first summary\n","        'categories': ', '.join  # Combine categories into a single string\n","    })\n","\n","    # Split categories into a list\n","    data['categories_list'] = data['categories'].str.split(', ')\n","\n","    # Combine summaries with joined categories for each row\n","    data['combined_text'] = data.apply(\n","        lambda row: row['summaries'] + \" \" + \" \".join(row['categories_list']),\n","        axis=1\n","    )\n","    return data\n","def generate_embeddings(data, model_name='all-MiniLM-L6-v2'):\n","    \"\"\"\n","    Generate embeddings for book summaries using a pre-trained SBERT model.\n","\n","    Args:\n","        data (pd.DataFrame): Preprocessed dataset with 'summaries' column.\n","        model_name (str): Name of the pre-trained SBERT model.\n","\n","    Returns:\n","        np.ndarray: Array of embeddings.\n","    \"\"\"\n","    model = SentenceTransformer(model_name)\n","    data['embeddings'] = data['summaries'].apply(lambda x: model.encode(x, convert_to_tensor=True))\n","    return model, data\n","def generate_pairs(data, num_samples=1000):\n","    pairs = []\n","    sampled_combinations = combinations(data.iterrows(), 2)\n","    for (idx1, row1), (idx2, row2) in sampled_combinations:\n","        # Compute Jaccard similarity\n","        common_categories = len(set(row1['categories_list']) & set(row2['categories_list']))\n","        total_categories = len(set(row1['categories_list']) | set(row2['categories_list']))\n","        jaccard_similarity = common_categories / total_categories\n","\n","\n","        # Reuse precomputed embeddings\n","        semantic_similarity = util.pytorch_cos_sim(row1['embeddings'], row2['embeddings']).item()\n","\n","        # Final similarity (weighted average)\n","        combined_similarity = 0.9 * semantic_similarity + 0.1 * jaccard_similarity\n","\n","        # Append the pair with book names\n","        pairs.append({\n","            \"book1\": row1['book_name'],  # Book name for text1\n","            \"book2\": row2['book_name'],  # Book name for text2\n","            \"text1\": row1['summaries'] ,\n","            \"text2\": row2['summaries'] ,\n","            \"similarity\": combined_similarity\n","        })\n","\n","    return pd.DataFrame(pairs)\n","# Define bins for similarity scores\n","def stratify_data(pairs_df, high_threshold=0.5, low_threshold=0.3, samples_per_bin=5000):\n","    \"\"\"\n","    Stratifies pairs_df into bins based on similarity scores and samples equally from each bin.\n","\n","    Args:\n","        pairs_df (pd.DataFrame): DataFrame containing the similarity scores.\n","        high_threshold (float): Threshold for high similarity.\n","        low_threshold (float): Threshold for low similarity.\n","        samples_per_bin (int): Number of samples to draw from each bin.\n","\n","    Returns:\n","        pd.DataFrame: Stratified and sampled DataFrame.\n","    \"\"\"\n","    # Define bins\n","    high_similarity = pairs_df[pairs_df['similarity'] >= high_threshold]\n","    moderate_similarity = pairs_df[(pairs_df['similarity'] < high_threshold) & (pairs_df['similarity'] >= low_threshold)]\n","    low_similarity = pairs_df[pairs_df['similarity'] < low_threshold]\n","\n","    # Sample equally from each bin\n","    high_sample = high_similarity.sample(min(len(high_similarity), samples_per_bin), random_state=42)\n","    moderate_sample = moderate_similarity.sample(min(len(moderate_similarity), samples_per_bin), random_state=42)\n","    low_sample = low_similarity.sample(min(len(low_similarity), samples_per_bin), random_state=42)\n","\n","    # Combine samples and shuffle\n","    stratified_data = pd.concat([high_sample, moderate_sample, low_sample])\n","    return shuffle(stratified_data, random_state=42)\n","\n","data = load_data('books_summary.csv')\n","data = preprocess_data(data)\n","# Create emnedding from pre-trained model for fine-tuning\n","model,data = generate_embeddings(data)\n","# Generate pairs\n","pairs_df = generate_pairs(data)\n","\n","# Apply stratification\n","stratified_pairs_df = stratify_data(pairs_df, samples_per_bin=5000)\n","\n","# Convert stratified pairs to InputExamples\n","train_examples = [\n","    InputExample(texts=[row['text1'], row['text2']], label=float(row['similarity']))\n","    for _, row in stratified_pairs_df.iterrows()\n","]\n","\n","# Create DataLoader\n","train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=128)\n","# Define the loss function\n","train_loss = losses.CosineSimilarityLoss(model)\n","\n","# Fine-tune the model\n","model.fit(\n","    train_objectives=[(train_dataloader, train_loss)],\n","    epochs=10,\n","    warmup_steps=int(0.01 * len(train_dataloader)),  # 10% warmup\n","    output_path=f'fine_tuned_sbert'\n",")"],"metadata":{"id":"8bF24Rkki4AL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tuning the sentence Bert model on book summaries"],"metadata":{"id":"caA2JdQvMG0a"}},{"cell_type":"code","source":["# Obtain embedding from fine-tuned model on preprocess data and\n","\n","def build_faiss_index(embeddings, index_path=\"faiss_index.bin\"):\n","    \"\"\"\n","    Build a Faiss index for fast nearest-neighbor searches and save it to a file.\n","\n","    Args:\n","        embeddings (np.ndarray): Array of book embeddings.\n","        index_path (str): Path to save the Faiss index.\n","\n","    Returns:\n","        faiss.Index: The built Faiss index.\n","    \"\"\"\n","    # Convert embeddings to float32 (required by Faiss)\n","    embeddings = embeddings.astype('float32')\n","\n","    # Create a Faiss index\n","    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance (Euclidean)\n","    index.add(embeddings)  # Add embeddings to the index\n","\n","    # Save the index\n","    faiss.write_index(index, index_path)\n","    return index\n","\n","data['book_name'] = data['book_name'].str.lower()\n","model = SentenceTransformer('fine_tuned_sbert')\n","embeddings = model.encode(data['summaries'], batch_size=16, show_progress_bar=True)\n","\n","# Build and save the Faiss index\n","faiss_index = build_faiss_index(embeddings, \"faiss_index.bin\")\n","# Save the new embeddings\n","np.save('book_embeddings.npy', embeddings)\n","# Save the pre-processed data\n","data.to_csv('preprocessed_books_data.csv', index=False)  #Save as CSV\n"],"metadata":{"id":"95CwuwtDmuc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZkygFifhnPHe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recommendation function"],"metadata":{"id":"ZuveP3ACMgcR"}},{"cell_type":"markdown","source":[],"metadata":{"id":"YUBYIcXLnKWc"}},{"cell_type":"markdown","source":["# Recommendations"],"metadata":{"id":"RsQ2BR6tjrKq"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import gradio as gr\n","from rapidfuzz import fuzz, process\n","\n","### Step 1: Load Processed Data ###\n","data = pd.read_csv('preprocessed_books_data.csv')  # Load preprocessed data\n","data['categories_list'] = data['categories_list'].apply(eval).apply(set)  # Convert categories to sets\n","\n","### Step 2: Embedding Loading ###\n","def load_embeddings(embedding_path):\n","    \"\"\"\n","    Load embeddings from a file.\n","    \"\"\"\n","    try:\n","        embeddings = np.load(embedding_path).astype('float32')  # Ensure float32 for Faiss compatibility\n","        return embeddings\n","    except Exception as e:\n","        raise FileNotFoundError(f\"Error loading embeddings: {e}\")\n","\n","embeddings = load_embeddings('book_embeddings.npy')\n","\n","### Step 3: Build or Load Faiss Index ###\n","def build_faiss_index(embeddings, index_path=\"faiss_index.bin\"):\n","    \"\"\"\n","    Build a Faiss index for fast nearest-neighbor searches and save it to a file.\n","    \"\"\"\n","    index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n","    index.add(embeddings)  # Add embeddings to the index\n","    faiss.write_index(index, index_path)  # Save the index\n","    return index\n","\n","def load_faiss_index(index_path=\"faiss_index.bin\"):\n","    \"\"\"\n","    Load a prebuilt Faiss index from a file.\n","    \"\"\"\n","    return faiss.read_index(index_path)\n","\n","try:\n","    faiss_index = load_faiss_index(\"faiss_index.bin\")\n","except FileNotFoundError:\n","    faiss_index = build_faiss_index(embeddings, \"faiss_index.bin\")\n","\n","### Step 4: Recommendation Function ###\n","def recommend_books_with_faiss(book_title, data, faiss_index, embeddings, top_n=5, min_similarity=60):\n","    \"\"\"\n","    Recommend books similar to the input book using Faiss for nearest-neighbor search.\n","    \"\"\"\n","    # Normalize book titles to lowercase\n","    book_title = book_title.lower()\n","\n","    # Fuzzy matching for book title\n","    if book_title not in data['book_name'].values:\n","        closest_match = process.extractOne(\n","            book_title,\n","            data['book_name'].values,\n","            scorer=fuzz.token_sort_ratio\n","        )\n","        if closest_match is None or closest_match[1] < min_similarity:\n","            return [f\"No close match found for '{book_title}'. Please try another title.\"], book_title\n","\n","        book_title = closest_match[0]\n","        print(f\"Giving results for: {book_title}\")\n","\n","    # Find the index of the input book\n","    input_idx = data[data['book_name'] == book_title].index[0]\n","    input_embedding = embeddings[input_idx].reshape(1, -1)  # Reshape for Faiss compatibility\n","\n","    # Use Faiss to find the nearest neighbors\n","    distances, indices = faiss_index.search(input_embedding, top_n + 1)  # +1 to exclude itself\n","    indices = indices.flatten()\n","    distances = distances.flatten()\n","\n","    # Exclude the input book itself\n","    indices = indices[1:]\n","    distances = distances[1:]\n","\n","    # Convert distances to cosine similarity\n","    cosine_similarities = 1 - (distances / 2)\n","\n","    # Filter by categories\n","    input_categories = data.loc[input_idx, 'categories_list']\n","    filtered_books = []\n","    for idx, sim in zip(indices, cosine_similarities):\n","        if len(input_categories & data.loc[idx, 'categories_list']) > 0:  # Category overlap\n","            filtered_books.append((data.loc[idx, 'book_name'], sim))\n","        if len(filtered_books) >= top_n:\n","            break\n","\n","    # Fallback: Add recommendations without category filtering\n","    if len(filtered_books) < top_n:\n","        remaining_indices = [idx for idx in indices if idx not in [rec[0] for rec in filtered_books]]\n","        for idx, sim in zip(remaining_indices, cosine_similarities[len(filtered_books):]):\n","            filtered_books.append((data.loc[idx, 'book_name'], sim))\n","            if len(filtered_books) >= top_n:\n","                break\n","\n","    return filtered_books[:top_n], book_title\n","\n","### Step 5: Recommendation UI ###\n","def recommend_ui(book_title):\n","    recommendations, book_name = recommend_books_with_faiss(book_title, data, faiss_index, embeddings, top_n=5)\n","\n","    if len(recommendations) < 2:\n","        return \"Book not found in the dataset. Please try another title.\"\n","\n","    output_message = f\"Giving results for: {book_name}\\n\\nRecommended Books:\\n\"\n","    recommendations_list = \"\\n\".join([f\"{rec[0]}\" for rec in recommendations])\n","    return output_message + recommendations_list\n","\n"],"metadata":{"id":"J2yOZnVtjtqc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommend_ui(book_title='1984')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"nnbG_F5kmyJ6","executionInfo":{"status":"ok","timestamp":1737962829667,"user_tz":-540,"elapsed":11,"user":{"displayName":"Manoj Rathor","userId":"15378941581879778136"}},"outputId":"749dec63-3442-484c-cefc-ec71b55324aa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Giving results for: 1984\\n\\nRecommended Books:\\nbrave new world\\nantifragile\\nthe sovereign individual\\ncommon sense\\nbrave new world'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":[],"metadata":{"id":"-BFVB52CnT31"},"execution_count":null,"outputs":[]}]}